
import pytest
import random

from solution import OptimizedForcedAlignmentCorrector


@pytest.fixture()
def corrector():
    return OptimizedForcedAlignmentCorrector()


class TestBasicFunctionality:
    """Basic functionality tests"""

    @pytest.mark.parametrize(
        "ground_truth,mfa,expected",
        [
            # 1. Single <unk> in middle
            ("æˆ‘ å–œæ­¡ å­¸ç¿’ æ¼”ç®—æ³•", "æˆ‘ å–œæ­¡ <unk> æ¼”ç®—æ³•", "æˆ‘ å–œæ­¡ å­¸ç¿’ æ¼”ç®—æ³•"),
            # 2. Multiple separated <unk>
            (
                "ä»Šå¤© å¤©æ°£ çœŸçš„ å¾ˆ å¥½",
                "ä»Šå¤© <unk> çœŸçš„ <unk> å¥½",
                "ä»Šå¤© å¤©æ°£ çœŸçš„ å¾ˆ å¥½",
            ),
            # 3. Consecutive <unk>
            ("æˆ‘ è¦ å» åŒ—äº¬ æ—…éŠ", "æˆ‘ <unk> <unk> åŒ—äº¬ æ—…éŠ", "æˆ‘ è¦ å» åŒ—äº¬ æ—…éŠ"),
            # 4. <unk> at start
            ("æˆ‘ å–œæ­¡ ç¨‹å¼ è¨­è¨ˆ", "<unk> å–œæ­¡ ç¨‹å¼ è¨­è¨ˆ", "æˆ‘ å–œæ­¡ ç¨‹å¼ è¨­è¨ˆ"),
            # 5. <unk> at end
            ("ä»Šå¤© å¤©æ°£ å¾ˆ å¥½", "ä»Šå¤© å¤©æ°£ å¾ˆ <unk>", "ä»Šå¤© å¤©æ°£ å¾ˆ å¥½"),
            # 6. All tokens are <unk>
            ("ç”² ä¹™ ä¸™ ä¸", "<unk> <unk> <unk> <unk>", "ç”² ä¹™ ä¸™ ä¸"),
            # 7. Repeated token ambiguity
            ("æˆ‘ å–œæ­¡ å–œæ­¡ ä½ ", "æˆ‘ <unk> å–œæ­¡ ä½ ", "æˆ‘ å–œæ­¡ å–œæ­¡ ä½ "),
        ],
    )
    def test_wildcard_matching_basic(self, corrector, ground_truth, mfa, expected):
        """Validate wildcard matching for typical scenarios"""
        assert corrector.correct_wildcard_matching(ground_truth, mfa) == expected


class TestEdgeCases:
    """Edge case tests"""

    def test_empty_strings(self, corrector):
        """Test with empty strings"""
        assert corrector.correct_wildcard_matching("", "") == ""

    def test_single_token(self, corrector):
        """Test with single token"""
        assert corrector.correct_wildcard_matching("æˆ‘", "æˆ‘") == "æˆ‘"
        assert corrector.correct_wildcard_matching("æˆ‘", "<unk>") == "æˆ‘"

    def test_only_unk_tokens(self, corrector):
        """Test when MFA has only <unk> tokens"""
        gt = "é€™ æ˜¯ ä¸€ å€‹ æ¸¬è©¦"
        mfa = "<unk> <unk> <unk> <unk> <unk>"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert result == gt

    def test_no_unk_tokens(self, corrector):
        """Test when MFA has no <unk> tokens"""
        text = "ä»Šå¤© å¤©æ°£ å¾ˆ å¥½"
        assert corrector.correct_wildcard_matching(text, text) == text

    def test_more_unk_than_ground_truth(self, corrector):
        """Test when there are more <unk> than ground truth tokens"""
        gt = "æˆ‘ æ„›"
        mfa = "<unk> <unk> <unk>"
        result = corrector.correct_wildcard_matching(gt, mfa)
        # Should handle gracefully without crashing
        assert len(result.split()) <= 3

    def test_alternating_unk(self, corrector):
        """Test alternating <unk> and normal tokens"""
        gt = "ä¸€ äºŒ ä¸‰ å›› äº”"
        mfa = "<unk> äºŒ <unk> å›› <unk>"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert result == "ä¸€ äºŒ ä¸‰ å›› äº”"


class TestComplexPatterns:
    """Complex pattern tests"""

    def test_multiple_consecutive_unk_groups(self, corrector):
        """Test multiple groups of consecutive <unk>"""
        gt = "æˆ‘ æ„› å­¸ ç¿’ ä¸­ æ–‡ å’Œ è‹± æ–‡"
        mfa = "æˆ‘ <unk> <unk> ç¿’ <unk> <unk> å’Œ è‹± æ–‡"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert "æ„›" in result and "å­¸" in result and "ä¸­" in result and "æ–‡" in result

    def test_long_consecutive_unk(self, corrector):
        """Test very long consecutive <unk> sequence"""
        gt = " ".join([f"å­—{i}" for i in range(10)])
        mfa = "å­—0 " + " ".join(["<unk>"] * 8) + " å­—9"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert result.startswith("å­—0") and result.endswith("å­—9")

    def test_repeated_patterns(self, corrector):
        """Test with repeated patterns in text"""
        gt = "æ¸¬è©¦ æ¸¬è©¦ ä¸€ äºŒ ä¸‰ æ¸¬è©¦ æ¸¬è©¦"
        mfa = "æ¸¬è©¦ <unk> ä¸€ äºŒ ä¸‰ <unk> æ¸¬è©¦"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert result.count("æ¸¬è©¦") == 4

    def test_ambiguous_context(self, corrector):
        """Test with ambiguous context that could match multiple positions"""
        gt = "æˆ‘ å–œæ­¡ ä½  æˆ‘ å–œæ­¡ ä»–"
        mfa = "æˆ‘ <unk> ä½  æˆ‘ å–œæ­¡ <unk>"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert "å–œæ­¡" in result and "ä»–" in result


class TestMixedContent:
    """Tests with mixed Chinese/English content"""

    def test_mixed_chinese_english(self, corrector):
        """Test with mixed Chinese and English tokens"""
        gt = "æˆ‘ love å­¸ç¿’ Python å’Œ æ©Ÿå™¨ learning"
        mfa = "æˆ‘ <unk> å­¸ç¿’ Python <unk> æ©Ÿå™¨ learning"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert "love" in result and "å’Œ" in result

    def test_english_only(self, corrector):
        """Test with English only text"""
        gt = "I love machine learning algorithms"
        mfa = "I <unk> machine <unk> algorithms"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert result == gt

    def test_numbers_and_symbols(self, corrector):
        """Test with numbers and symbols"""
        gt = "åƒ¹æ ¼ æ˜¯ 100 å…ƒ æˆ– $20"
        mfa = "åƒ¹æ ¼ <unk> 100 å…ƒ <unk> $20"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert "æ˜¯" in result and "æˆ–" in result


class TestPerformance:
    """Performance and stress tests"""

    def test_large_text(self, corrector):
        """Test with large text (near maximum constraint)"""
        tokens = [f"å­—{i}" for i in range(500)]
        gt = " ".join(tokens)

        # Replace every 10th token with <unk>
        mfa_tokens = tokens.copy()
        for i in range(0, len(mfa_tokens), 10):
            mfa_tokens[i] = "<unk>"
        mfa = " ".join(mfa_tokens)

        result = corrector.correct_wildcard_matching(gt, mfa)
        assert len(result.split()) == len(tokens)

    def test_many_unk(self, corrector):
        """Test with many <unk> tokens (50% of text)"""
        gt = " ".join([f"å­—{i}" for i in range(100)])
        tokens = gt.split()
        mfa_tokens = []
        for i, token in enumerate(tokens):
            if i % 2 == 0:
                mfa_tokens.append("<unk>")
            else:
                mfa_tokens.append(token)
        mfa = " ".join(mfa_tokens)

        result = corrector.correct_wildcard_matching(gt, mfa)
        result_tokens = result.split()

        # Check that non-<unk> tokens are preserved
        for i in range(1, len(tokens), 2):
            assert result_tokens[i] == tokens[i]


class TestIdempotence:
    """Idempotence and consistency tests"""

    def test_idempotent_correction(self, corrector):
        """Running correction twice should not change the result"""
        gt = "æˆ‘ è¦ å» åŒ—äº¬ æ—…éŠ"
        mfa = "æˆ‘ <unk> <unk> åŒ—äº¬ æ—…éŠ"
        once = corrector.correct_wildcard_matching(gt, mfa)
        twice = corrector.correct_wildcard_matching(gt, once)
        assert once == twice

    def test_already_correct(self, corrector):
        """Correcting already correct text should not change it"""
        text = "é€™ æ˜¯ æ­£ç¢º çš„ æ–‡æœ¬"
        assert corrector.correct_wildcard_matching(text, text) == text


class TestAllMethods:
    """Test all correction methods"""

    @pytest.mark.parametrize(
        "method_name",
        [
            "correct_wildcard_matching",
            "correct_kmp_variant",
            "correct_rolling_hash",
            "correct_suffix_array",
            "correct_with_fsa",
        ],
    )
    def test_method_consistency(self, corrector, method_name):
        """All methods should handle basic case without crashing"""
        gt = "æˆ‘ å–œæ­¡ å­¸ç¿’ æ¼”ç®—æ³•"
        mfa = "æˆ‘ å–œæ­¡ <unk> æ¼”ç®—æ³•"

        method = getattr(corrector, method_name)
        result = method(gt, mfa)

        # Should not crash and should return something
        assert result is not None
        assert isinstance(result, str)

        # Should have same number of tokens or close to it
        result_tokens = result.split()
        gt_tokens = gt.split()
        assert abs(len(result_tokens) - len(gt_tokens)) <= 2

    @pytest.mark.parametrize(
        "method_name",
        [
            "correct_wildcard_matching",
            "correct_kmp_variant",
            "correct_rolling_hash",
            "correct_suffix_array",
            "correct_with_fsa",
        ],
    )
    def test_method_no_unk(self, corrector, method_name):
        """All methods should handle text without <unk> correctly"""
        text = "é€™ æ˜¯ æ¸¬è©¦ æ–‡æœ¬"
        method = getattr(corrector, method_name)
        assert method(text, text) == text


class TestRobustness:
    """Robustness and error handling tests"""

    def test_malformed_input(self, corrector):
        """Test with malformed input"""
        # Extra spaces
        gt = "æˆ‘  å–œæ­¡   å­¸ç¿’"
        mfa = "æˆ‘   <unk>  å­¸ç¿’"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert result is not None

    def test_special_characters(self, corrector):
        """Test with special characters"""
        gt = "é€™æ˜¯ã€Œå¼•è™Ÿã€å’Œï¼ˆæ‹¬è™Ÿï¼‰"
        mfa = "é€™æ˜¯ã€Œ<unk>ã€å’Œï¼ˆæ‹¬è™Ÿï¼‰"
        result = corrector.correct_wildcard_matching(gt, mfa)
        # Current algorithm can't infer inside paired punctuation; just ensure no crash
        assert result.startswith("é€™æ˜¯")

    def test_unicode_characters(self, corrector):
        """Test with various Unicode characters"""
        gt = "ğŸ˜€ è¡¨æƒ… ç¬¦è™Ÿ â™  â™¥ â™¦ â™£"
        mfa = "ğŸ˜€ <unk> ç¬¦è™Ÿ â™  <unk> â™¦ â™£"
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert "è¡¨æƒ…" in result and "â™¥" in result


class TestFuzzing:
    """Fuzzing tests with random inputs"""

    @pytest.mark.parametrize("seed", [42, 123, 456, 789, 1001])
    def test_random_unk_placement(self, corrector, seed):
        """Test with random <unk> placement"""
        random.seed(seed)

        # Generate random ground truth
        tokens = [f"å­—{i}" for i in range(random.randint(10, 50))]
        gt = " ".join(tokens)

        # Randomly replace some tokens with <unk>
        mfa_tokens = tokens.copy()
        num_unk = random.randint(1, len(tokens) // 2)
        unk_positions = random.sample(range(len(tokens)), num_unk)

        for pos in unk_positions:
            mfa_tokens[pos] = "<unk>"
        mfa = " ".join(mfa_tokens)

        # Should not crash
        result = corrector.correct_wildcard_matching(gt, mfa)
        assert result is not None
        assert len(result.split()) == len(tokens)

    def test_stress_consecutive_unk(self, corrector):
        """Stress test with many consecutive <unk>"""
        for num_consecutive in [2, 5, 10, 20]:
            tokens = [f"å­—{i}" for i in range(30)]
            gt = " ".join(tokens)

            # Place consecutive <unk> in the middle
            start = 10
            mfa_tokens = (
                tokens[:start]
                + ["<unk>"] * num_consecutive
                + tokens[start + num_consecutive :]
            )
            mfa = " ".join(mfa_tokens)

            result = corrector.correct_wildcard_matching(gt, mfa)
            assert result is not None


# Benchmark fixture for performance testing
@pytest.fixture
def benchmark_data():
    """Generate benchmark test data"""
    return [
        # Small text
        ("æˆ‘ å–œæ­¡ å­¸ç¿’", "æˆ‘ <unk> å­¸ç¿’"),
        # Medium text
        (
            " ".join([f"å­—{i}" for i in range(50)]),
            " ".join([f"å­—{i}" if i % 5 != 0 else "<unk>" for i in range(50)]),
        ),
        # Large text
        (
            " ".join([f"å­—{i}" for i in range(200)]),
            " ".join([f"å­—{i}" if i % 10 != 0 else "<unk>" for i in range(200)]),
        ),
    ]


@pytest.mark.parametrize(
    "method_name",
    [
        "correct_wildcard_matching",
        "correct_kmp_variant",
        "correct_rolling_hash",
        "correct_suffix_array",
        "correct_with_fsa",
    ],
)
def test_method_performance(corrector, benchmark_data, method_name):
    """Basic performance test for all methods"""
    import time

    method = getattr(corrector, method_name)
    total_time = 0

    for gt, mfa in benchmark_data:
        start = time.perf_counter()
        result = method(gt, mfa)
        elapsed = time.perf_counter() - start
        total_time += elapsed

        # Basic validation
        assert result is not None
        assert isinstance(result, str)

    # Just ensure it completes in reasonable time (< 1 second for all test cases)
    assert total_time < 1.0, f"{method_name} took too long: {total_time:.3f}s"
